{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저를 이용한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7023224c42b0>>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\"I've been waiting for long time\", \"I hate this so much!\"]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 살펴보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 헤드는 hidden states의 고차원 벡터를 입력으로 받아 다른 차원에 투영합니다. 일반적으로 헤드는 하나 또는 몇 개의 선형 레이어로 구성됩니다. \n",
    "\n",
    "## 2장 요약\n",
    "\n",
    "토크나이저의 작동 방식과 토큰화, 입력 식별자로의 변환, 패딩, 절단 및 어텐션 마스크등을 살펴보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "\n",
    "# 해당 시퀀스를 리스트 내의 최대 시퀀스 길이까지 패딩(padding) 합니다.\n",
    "# model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "# 시퀀스를 모델 최대 길이(model max length)까지 패딩(padding) 합니다.\n",
    "# (512 for BERT or DistilBERT)\n",
    "# model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 파인 튜닝\n",
    "\n",
    "기존에 사전 학습된 모델을 사용해 예측을 수행하는 방법을 살펴보았고, 여러분들이 가지고 있는 데이터셋을 가지고 파인튜닝을 하려면 어떻게 해야 할까요?\n",
    "\n",
    "이 섹션에서는 William B. Dolan과 Chris Brockett의 논문에서 소개된 MRPC(Microsoft Research Paraphrase Corpus) 데이터셋을 예제로 사용할 것입니다. 이 데이터셋은 5,801건의 문장 쌍으로 구성되어 있으며 각 문장 쌍의 관계가 의역(paraphrasing) 관계인지 여부를 나타내는 레이블이 존재합니다(즉, 두 문장이 동일한 의미인지 여부). 데이터셋의 규모가 그리 크지 않기 때문에 학습 과정을 쉽게 실험할 수 있습니다.\n",
    "\n",
    "Datasets 라이브러리는 허브(hub)에서 데이터셋을 다운로드하고 캐시(cache) 기능을 수행하는 쉬운 명령어를 제공합니다. 다음과 같이 MRPC 데이터셋을 다운로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"glue\", \"mrpc\"\n",
    ")  #  MRPC 데이터셋은 10개의 데이터셋으로 구성된 GLUE 벤치마크 중 하나.\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블(label)은 ClassLabel 타입이고 레이블 이름에 대한 정수 매핑은 names 폴더에 저장되어 있습니다. 0은 not_equivalent를 의미하고, 1은 equivalent를 나타냅니다.\n",
    "\n",
    "## 데이터셋 전처리\n",
    "\n",
    "데이터셋 전처리를 위해서는 우선적으로 텍스트를 모델이 이해할 수 있는 숫자로 변환해야 합니다. 이전 장에서 보았듯이 이는 토크나이저가 담당합니다. 토크나이저에 단일 문장 또는 다중 문장 리스트를 입력할 수 있으므로, 다음과 같이 각 쌍의 모든 첫 번째 문장과 두 번째 문장을 각각 직접 토큰화할 수 있습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 408/408 [00:00<00:00, 11558.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 해야 할 일은 전체 요소들을 배치(batch)로 분리할 때 가장 긴 요소의 길이로 모든 예제를 채우는(padding) 것입니다. 이를 동적 패딩(dynamic padding)이라고 합니다.\n",
    "\n",
    "## 동적 패딩(Dynamic padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Training)\n",
    "\n",
    "Trainer를 정의하기 전에 먼저 수행할 단계는 Trainer가 학습 및 평가에 사용할 모든 하이퍼파라미터(hyperparameters)를 포함하는 TrainingArguments 클래스를 정의하는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.30944098124230524, metrics={'train_runtime': 38.2818, 'train_samples_per_second': 287.447, 'train_steps_per_second': 35.97, 'total_flos': 405114969714960.0, 'train_loss': 0.30944098124230524, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "# 클래스 정의\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "# 모델 정의\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# trainer 정의\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 (Evaluation)\n",
    "유용한 compute_metrics() 함수를 구현하고 이를 학습할 때 사용하는 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460902</td>\n",
       "      <td>0.732843</td>\n",
       "      <td>0.835596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.451315</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.913495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368500</td>\n",
       "      <td>0.563403</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.905594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.40370349426934404, metrics={'train_runtime': 42.747, 'train_samples_per_second': 257.421, 'train_steps_per_second': 32.213, 'total_flos': 405114969714960.0, 'train_loss': 0.40370349426934404, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer class안쓰고 같은 결과를 얻는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/1377 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8382352941176471, 'f1': 0.89}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장. 모델 및 토크나이저 공유\n",
    "\n",
    "# 5장. Datasets 라이브러리\n",
    "\n",
    "## 로컬 데이터셋 로딩하기\n",
    "\n",
    "여기서는 예제 데이터로 이탈리아어 질의응답(question answering)을 위한 대규모 데이터셋인 SQuAD-it dataset을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 161297 examples [00:00, 373782.24 examples/s]\n",
      "Generating test split: 53766 examples [00:00, 361096.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"../input/drugsComTrain_raw.tsv\", \"test\": \"../input/drugsComTest_raw.tsv\"}\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 슬라이싱(slicing)과 다이싱(dicing)\n",
    "\n",
    "Pandas와 유사하게 🤗Datasets는 Dataset 및 DatasetDict 객체의 내용을 조작하는 여러 기능을 제공합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# 앞쪽의 샘플 몇개를 가져옵니다.\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재현성(reproducibility)을 위해 Dataset.shuffle()의 seed를 고정했습니다. Dataset.select()는 반복 가능 인덱스(iterable indices)를 매개변수로 입력해야 하므로 데이터셋에서 처음 1,000개의 예제를 가져오기 위해 range(1000)을 전달했습니다. 이 샘플들에서 우리는 해당 데이터셋의 몇 가지 단점(quirks)을 볼 수 있습니다:\n",
    "\n",
    "- \"Unnamed: 0\" 컬럼(column)은 확실하지는 않지만 각 환자의 익명 ID(anonymized ID)처럼 보입니다.\n",
    "- \"condition\" 컬럼(column)에는 대문자와 소문자 레이블이 혼합되어 있습니다.\n",
    "- 리뷰(review)의 길이는 다양하며 Python 줄 구분 기호(\\r\\n)와 '와 같은 HTML 문자 코드가 혼합되어 있습니다.\n",
    "\n",
    "🤗Datasets를 사용하여 이러한 문제들을 처리하는 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 161297/161297 [00:00<00:00, 328154.86 examples/s]\n",
      "Filter: 100%|██████████| 53766/53766 [00:00<00:00, 331288.33 examples/s]\n",
      "Map: 100%|██████████| 160398/160398 [00:06<00:00, 26529.89 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:02<00:00, 26483.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "# 결측치 제거\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# 소문자화가 제대로 진행되었는지 확인.\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 컬럼(column) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [00:04<00:00, 33152.89 examples/s]\n",
      "Map: 100%|██████████| 53471/53471 [00:01<00:00, 32796.55 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# 첫 학습 예제를 살펴봅니다.\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 160398/160398 [00:00<00:00, 296854.58 examples/s]\n",
      "Filter: 100%|██████████| 53471/53471 [00:00<00:00, 303639.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset.filter() 함수를 사용하여 30단어 미만으로 표현된 리뷰를 제거해 보겠습니다.\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138514/138514 [00:02<00:00, 47611.16 examples/s]\n",
      "Map: 100%|██████████| 46108/46108 [00:00<00:00, 47460.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 리뷰에 HTML 문자 코드가 있다는 것입니다. Python의 html 모듈을 사용하여 다음과 같이 이러한 문자를 이스케이프 해제(unescape)할 수 있습니다\n",
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])},\n",
    "                                batched=True # 속도를 위해\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증 집합(validation set) 생성\n",
    "모델 성능 평가에 사용할 수 있는 평가 집합(test set)이 있지만 개발 중에 이 평가 집합을 그대로 두고 별도의 검증 집합(validation set)을 만드는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# 기본 \"test\" 분할을 \"validation\"으로 변경함.\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# 'DatasetDict'에 \"test\" 집합을 추가.\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets으로 빅데이터 문제를 해결\n",
    "\n",
    "### pile이 무엇인가?\n",
    "\n",
    "Pile은 [EleutherAI](https://www.eleuther.ai/)가 대규모 언어 모델을 학습하기 위해 만든 영어 텍스트 말뭉치입니다. 여기에는 학술 논문(scientific articles), GitHub 코드 리포지토리 및 필터링된 웹 텍스트에 이르는 다양한 데이터셋이 포함되어 있습니다. 학습 데이터는 [14GB 청크](https://the-eye.eu/public/AI/pile/)로 제공되며 [개별적인 구성 요소](https://the-eye.eu/public/AI/pile_preliminary_components/)를 각각 다운로드할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_dataset = load_dataset(\"casinca/PUBMED_title_abstracts_2019_baseline\", split=\"train\", streaming=True)\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age. Systematic review of the published literature. Out-patient clinics, emergency departments and hospitalisation wards in 23 health centres from 10 countries. Cohort studies reporting the frequency of hypoxaemia in children under 5 years of age with ALRI, and the association between hypoxaemia and the risk of dying. Prevalence of hypoxaemia measured in children with ARI and relative risks for the association between the severity of illness and the frequency of hypoxaemia, and between hypoxaemia and the risk of dying. Seventeen published studies were found that included 4,021 children under 5 with acute respiratory infections (ARI) and reported the prevalence of hypoxaemia. Out-patient children and those with a clinical diagnosis of upper ARI had a low risk of hypoxaemia (pooled estimate of 6% to 9%). The prevalence increased to 31% and to 43% in patients in emergency departments and in cases with clinical pneumonia, respectively, and it was even higher among hospitalised children (47%) and in those with radiographically confirmed pneumonia (72%). The cumulated data also suggest that hypoxaemia is more frequent in children living at high altitude. Three papers reported an association between hypoxaemia and death, with relative risks varying between 1.4 and 4.6. Papers describing predictors of hypoxaemia have focused on clinical signs for detecting hypoxaemia rather than on identifying risk factors for developing this complication. Hypoxaemia is a common and potentially lethal complication of ALRI in children under 5, particularly among those with severe disease and those living at high altitude. Given the observed high prevalence of hypoxaemia and its likely association with increased mortality, efforts should be made to improve the detection of hypoxaemia and to provide oxygen earlier to more children with severe ALRI.'},\n",
       " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
       "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy.\\nHypoxaemia is a common complication of acute lower respiratory tract infections in children. In most developing countries, where the majority of deaths from pneumonia occur, facilities for early detection of hypoxaemia are lacking and oxygen is in short supply. This review examines the usefulness of different clinical signs and symptoms in the prediction of hypoxaemia associated with acute respiratory infections in children. Several respiratory signs were found to be associated with hypoxaemia. These include very fast breathing (with a respiratory rate of more than 60 or 70 breaths per minute), cyanosis, grunting, nasal flaring, chest retractions, head nodding and auscultatory signs, as well as signs of general depression of the child, such as inability to feed or lethargy. The sensitivity and specificity of these signs, as described in the reviewed studies, is presented, and combination rules are discussed. Through appropriate combination of several physical signs, which can be used by peripheral health workers and be taught to mothers, it is possible to predict hypoxaemia in children with acute respiratory tract infections with reasonable accuracy.'},\n",
       " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
       "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea.\\nTo investigate the severity and duration of hypoxaemia in 703 children with severe or very severe pneumonia presenting to Goroka Hospital in the Papua New Guinea highlands; to study the predictive value of clinical signs for the severity of hypoxaemia, the predictive value of transcutaneous oxygen saturation (SpO2) and other variables for mortality. Prospective evaluation of children with severe or very severe pneumonia. SpO2 was measured at the time of presentation and every day until hypoxaemia resolved. Children with a SpO2 less than 85% received supplemental oxygen. By comparing with a retrospective control group for whom oxygen administration was guided by clinical signs, we evaluated whether there was a survival advantage from using a protocol for the administration of oxygen based on pulse oximetry. We determined normal values for oxygen saturation in children living in the highlands. In 151 well, normal highland children, the mean SpO2 was 95.7% (SD 2.7%). The median SpO2 among children with severe or very severe pneumonia was 70% (56-77); 376 (53.5%) had moderate hypoxaemia (SpO2 70-84%); 202 (28.7%) had severe hypoxaemia (SpO2 50-69%); and 125 (17.8%) had very severe hypoxaemia (SpO2 < 50%). Longer duration of cough or the presence of hepatomegaly or cyanosis predicted more severe degrees of hypoxaemia. After 10, 20 and 30 days from the beginning of treatment, respectively 102 (14.5%), 38 (5.4%) and 19 (2.7%) of children had persistent hypoxaemia; 46 children (6.5%) died. Predictors of death were low SpO2 on presentation, severe malnutrition, measles and history of cough for more than 7 days. The mortality risk ratio between the 703 children managed whose oxygen administration was guided by the use of pulse oximetry and the retrospective control group who received supplemental oxygen based on clinical signs was 0.65 (95%CI 0.41-1.02, two-sided Fisher's exact test, P = 0.07). There is a need to increase the availability of supplemental oxygen in smaller health facilities in developing countries, and to train health workers to recognise the clinical signs and risk factors for hypoxaemia. In moderate sized hospitals a protocol for the administration of oxygen based on pulse oximetry may improve survival.\"},\n",
       " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
       "  'text': 'Oxygen concentrators and cylinders.\\nA comparison is made between oxygen cylinders and oxygen concentrators as sources for clinical use. Oxygen cylinders are in widespread use, but costs and logistic factors favour the use of concentrators in many developing country situations, especially where cylinder supplies fail to penetrate.'},\n",
       " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
       "  'text': 'Oxygen supply in rural africa: a personal experience.\\nOxygen is one of the essential medical supplies in any hospital setting. However, in some rural African settings without regular electricity and with logistical problems in the transport of oxygen cylinders, the delivery of oxygen can be difficult. This paper compares the costs incurred using oxygen cylinders with a solar operated oxygen concentrator in a rural Gambian setting. The paper shows that a solar operated system has a high capital investment initially, but that running costs and maintenance are minimal. The system becomes cost effective if a rural hospital needs more than 6 treatment days (1 l/min) of oxygen per month and can be considered in a setting where 6 hours of sunlight per day can be guaranteed.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = pubmed_dataset.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자신만의 데이터셋 만들기\n",
    "\n",
    "일단은 스킵한다.\n",
    "\n",
    "## FAISS를 이용한 시맨틱 검색\n",
    "\n",
    "### 시맨틱 검색을 위한 임베딩 사용하기\n",
    "1장에서 보았듯이 트랜스포머(Transformer) 기반 언어 모델은 텍스트 내의 각 토큰을 임베딩 벡터(embedding vector) 로 나타냅니다. 개별 임베딩을 \"풀링(pooling)\"하여 전체 문장, 단락 또는 (경우에 따라) 문서에 대한 벡터 표현을 생성할 수 있습니다. 그런 다음 이러한 임베딩을 사용하여 각 임베딩 사이의 내적 유사도(dot-product similarity), 또는 다른 유사도 메트릭(similarity metric)을 계산하고 가장 많이 겹치는 문서를 반환하여 코퍼스에서 유사 문서 검색을 수행할 수 있습니다.\n",
    "\n",
    "이 섹션에서는 임베딩을 사용하여 시맨틱 검색 엔진을 개발할 것입니다. 이러한 검색 엔진은 쿼리와 문서의 키워드 매칭을 기반으로 하는 기존 접근 방식에 비해 몇 가지 장점을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"../input/drugsComTrain_raw.tsv\", \"test\": \"../input/drugsComTest_raw.tsv\"}\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "# 결측치 제거\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['drugName', 'condition'],\n",
       "    num_rows: 160398\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset.remove_columns() 함수를 사용하여 그외의 나머지 열을 삭제해 보겠습니다:\n",
    "dataset = drug_dataset[\"train\"]\n",
    "columns = dataset.column_names\n",
    "columns_to_keep = [\"drugName\", \"condition\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"drugName\"] \n",
    "        + \" \\n \" \n",
    "        + examples[\"condition\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(concatenate_text)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embedding = get_embeddings(dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160398/160398 [07:32<00:00, 354.41 examples/s]\n",
      "100%|██████████| 161/161 [00:00<00:00, 408.53it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'comments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16936/2907470400.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m embeddings_dataset = dataset.map(\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0membeddings_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the best drug in each condition?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'comments'"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "question = \"What is the best drug in each condition?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "# question_embedding.shape\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers 라이브러리\n",
    "\n",
    "- 새로운 텍스트 말뭉치를 기반으로 특정 체크포인트의 토크나이저와 유사한 새로운 토크나이저를 학습시키는 방법\n",
    "- 오늘날 NLP에서 사용되는 세 가지 주요 단어 토큰화 알고리즘의 차이점\n",
    "- 🤗Tokenizers 라이브러리를 사용하여 처음부터 토크나이저를 구축하고 특정 데이터로 학습하는 방법\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 토크나이저에서 새로운 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 로드하는데 몇 분이 소요될 수 있습니다. 커피나 차를 준비하세요.\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# 이제 텍스트 배치(batch)의 이터레이터(iterator) 형태로 말뭉치를 구성\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "\n",
    "# 모델과 일치시키려는 토크나이저를 로드해야\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# train_new_from_iterator() 사용 중인 토크나이저가 \"빠른(fast)\" 토크나이저인 경우에만 작동합\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "# tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old tokenizer result:\n",
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n",
      "\n",
      "New tokenizer result:\n",
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "old_tokens = old_tokenizer.tokenize(example)\n",
    "new_tokens = tokenizer.tokenize(example)\n",
    "\n",
    "print(\"Old tokenizer result:\")\n",
    "print(old_tokens)\n",
    "print(\"\\nNew tokenizer result:\")\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"빠른(fast)\" 토크나이저의 특별한 능력\n",
    "\n",
    "### 배치 인코딩 (Batch encoding)\n",
    "\n",
    "### 입력(inputs)에서 예측(predictions)까지\n",
    "\n",
    "### 엔터티 그룹화\n",
    "\n",
    "## QA 파이프라인에서의 \"빠른(fast)\" 토크나이저\n",
    "\n",
    "질의 응답(question answering) 작업에 관심이 없다면 이 섹션을 건너뛸 수 있습니다.\n",
    "\n",
    "## 정규화(Normalization) 및 사전 토큰화(Pre-tokenization)\n",
    "\n",
    "텍스트를 하위 토큰(subtokens)으로 분할하기 전에(모델에 따라), 토크나이저는 정규화(normalization) 및 사전 토큰화(pre-tokenization) 두 단계를 수행합니다.\n",
    "\n",
    "### 정규화(Normalization)\n",
    "정규화 단계에는 불필요한 공백 제거, 소문자 변환(lowercasing) 및 악센트 제거 등과 같은 몇가지 일반적인 정제 작업이 포함됩니다.\n",
    "\n",
    "### 사전토큰화(Pre-tokenization)\n",
    "\n",
    "토크나이저는 원시 텍스트만으로는 학습될 수 없습니다. 대신에 먼저 텍스트를 단어와 같은 작은 개체들로 분할해야 합니다. 여기서 사전 토큰화(pre-tokenization) 단계가 실행됩니다. \n",
    "\n",
    "### SentencePiece\n",
    "\n",
    "텍스트 전처리를 위한 토큰화 알고리즘입니다. 텍스트를 일련의 유니코드 문자들로 간주하고 공백을 특수 문자인 _로 치환합니다. Unigram 알고리즘(섹션 7 참조)과 함께 사용하면 사전 토큰화(pre-tokenization) 단계가 필요하지 않으므로 공백 문자가 사용되지 않는 언어(예: 중국어 또는 일본어)에 매우 유용합니다.\n",
    "\n",
    "## Byte-Pair Encoding (BPE) 토큰화\n",
    "\n",
    " 토큰화 알고리즘에 대한 일반적인 개요만을 원하는 경우 이 장을 건너뛰어도 됩니다.\n",
    "\n",
    "## WordPiece 토큰화\n",
    "\n",
    " WordPiece를 심층적으로 다루며 전체 구현 과정을 보여줍니다. 토큰화 알고리즘에 대한 일반적인 개요를 원하는 경우 생략해도 좋습니다.\n",
    "\n",
    " ## Unigram 토큰화\n",
    "\n",
    "  토큰화 알고리즘에 대한 일반적인 개요를 원하는 경우 생략해도 좋습니다.\n",
    "\n",
    "## 블록 단위로 토크나이저 빌딩하기\n",
    "\n",
    "\n",
    " 토큰화는 다음과 같은 단계로 실행됩니다:\n",
    "\n",
    "- 정규화 (공백이나 악센트 제거, 유니코드 정규화 등과 같이 필요하다고 여겨지는 모든 텍스트 정제 작업)\n",
    "- 사전 토큰화 (입력을 단어들로 분리)\n",
    "- 모델을 통한 입력 실행 (사전 토큰화된 단어들을 사용하여 토큰 시퀀스 생성)\n",
    "- 후처리 (토큰나이저의 특수 토큰 추가, attention mask 및 토큰 유형 ID 생성)\n",
    "\n",
    "\n",
    "### 말뭉치 확보\n",
    "\n",
    "새로운 토크나이저를 학습하기 위해 작은 텍스트 말뭉치를 사용합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 337165.93 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 530895.64 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1183711.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "# WordPiece 토크나이저를 처음부터 빌딩하기\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "# 토큰화의 첫 번째 단계는 정규화(normalization)입니다.\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "# 다음은 사전 토큰화 단계입니다\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 모든 특수 토큰을 전달해야 한다는 것\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "# 앞에서 정의한 반복자(iterator)를 사용하여 모델을 학습\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "# 테스트\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer\")\n",
    "print(encoding.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주요 NLP 태스크 실제 구현방법\n",
    "\n",
    "다음과 같은 주요 NLP 태스크들을 다룰 것입니다.\n",
    "- 토큰 분류 (Token Classification)\n",
    "- 마스킹된 언어 모델링 (Masked Language Modeling)\n",
    "- 요약 (Summarization)\n",
    "- 번역 (Translation)\n",
    "- 인과적 언어 모델링 사전학습 (Causal Language Modeling Pretraining like GPT-2)\n",
    "- 질의응답 (Question Answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 분류 (Token Classification)\n",
    "\n",
    "이 포괄적인 작업은 다음과 같이 \"문장의 각 토큰에 레이블을 지정\"하는 것으로 정형화될 수 있는 모든 문제를 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14041/14041 [00:00<00:00, 36183.05 examples/s]\n",
      "Map: 100%|██████████| 3250/3250 [00:00<00:00, 41512.59 examples/s]\n",
      "Map: 100%|██████████| 3453/3453 [00:00<00:00, 26093.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# 데이터 처리\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 새로운 단어의 시작 토큰.\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 특수 토큰.\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 이전 토큰과 동일한 단어에 소속된 토큰.\n",
    "            label = labels[word_id]\n",
    "            # 만약 레이블이 B-XXX이면 이를 I-XXX로 변경.\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Trainer API를 이용하여 모델 미세조정\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 기준 (Metrics) \n",
    "\n",
    "이후 생략\n",
    "\n",
    "## 마스크 언어 모델(Masked Language Model) 미세조정\n",
    "\n",
    "분야 특화 데이터에 대해 사전 학습된 언어 모델을 미세 조정하는 이 프로세스를 일반적으로 도메인 어뎁테이션(domain adaptation)이라고 합니다. \n",
    "\n",
    "### 마스크 언어 모델링(MLM)을 위해 사전학습된 모델(pretrained model) 선택하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 200707.83 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 442808.94 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 515089.78 examples/s]\n",
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 25000/25000 [00:02<00:00, 11886.86 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 12986.14 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:04<00:00, 12211.42 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:19<00:00, 1264.06 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:18<00:00, 1327.32 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:40<00:00, 1249.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 마스크 토큰을 예측하려면 모델에 대한 입력을 생성하기 위해 DistilBERT의 토크나이저가 필요\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "#데이터셋\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 데이터 전처리\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# 빠른 멀티스레딩을 작동시키기 위해서, batched=True를 지정합니다.\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "# 메모리에 들어갈 수 있는 약간 작은 수치를 선택\n",
    "chunk_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # 모든 텍스트들을 결합한다.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 결합된 텍스트들에 대한 길이를 구한다.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # `chunk_size`보다 작은 경우 마지막 청크를 삭제\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # max_len 길이를 가지는 chunk 단위로 슬라이스\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 새로운 레이블 컬럼을 생성\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result                            \n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "# Trainer API를 이용하여 DistilBERT 미세조정\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 50_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 11.06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 02:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.496500</td>\n",
       "      <td>2.333867</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.424800</td>\n",
       "      <td>2.296707</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.391800</td>\n",
       "      <td>2.276380</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 9.97\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    # overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.train()\n",
    "\n",
    "eval_results  = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
