----


----


# Appendix A: Introduction to PyTorch (Part 1)

- https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-A/01_main-chapter-code/code-part1.ipynb

```{python}
import torch
print(torch.__version__)
```

## minimal example

```{python}
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Tuple

# GPU 사용 가능 여부 확인
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 데이터 정의
train_features = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
], device=device)

train_labels = torch.tensor([0, 0, 0, 1, 1], device=device)

test_features = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
], device=device)

test_labels = torch.tensor([0, 1], device=device)

# Dataloader 준비
class ToyDataset(Dataset):
    def __init__(self, features: torch.Tensor, labels: torch.Tensor):
        self.features = features
        self.labels = labels

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.features[index], self.labels[index]

    def __len__(self) -> int:
        return len(self.labels)

train_dataset = ToyDataset(train_features, train_labels)
test_dataset = ToyDataset(test_features, test_labels)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=2,
    shuffle=True,
    num_workers=0
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=2,
    shuffle=False,
    num_workers=0
)

# 모델 정의
class NeuralNetwork(torch.nn.Module):
    def __init__(self, input_size: int, output_size: int):
        super().__init__()

        self.layers = torch.nn.Sequential(
            torch.nn.Linear(input_size, 30),
            torch.nn.ReLU(),
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            torch.nn.Linear(20, output_size),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)

# 평가 지표 정의
def compute_accuracy(model: torch.nn.Module, dataloader: DataLoader) -> float:
    model.eval()
    correct_predictions = 0
    total_samples = 0
    
    with torch.no_grad():
        for features, labels in dataloader:
            features, labels = features.to(device), labels.to(device)
            logits = model(features)
            predictions = torch.argmax(logits, dim=1)
            correct_predictions += torch.sum(labels == predictions)
            total_samples += len(labels)

    return (correct_predictions / total_samples).item()

# 모델 초기화 및 GPU로 이동
model = NeuralNetwork(input_size=2, output_size=2).to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

# 학습 루프
num_epochs = 3

for epoch in range(num_epochs):
    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):
        features, labels = features.to(device), labels.to(device)
        
        logits = model(features)
        loss = F.cross_entropy(logits, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx+1:03d}/{len(train_loader):03d}"
              f" | Train Loss: {loss:.2f}")

    model.eval()

# 정확도 출력
test_accuracy = compute_accuracy(model, test_loader)
print(f"Test Accuracy: {test_accuracy:.2f}")

# 모델 저장 및 불러오기
# torch.save(model.state_dict(), "model.pth")
# loaded_model = NeuralNetwork(input_size=2, output_size=2).to(device)
# loaded_model.load_state_dict(torch.load("model.pth", map_location=device))
```

## Training with multiple GPUs

```{python}
import os
import platform
from typing import Tuple, List

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group


def ddp_setup(rank: int, world_size: int) -> None:
    """
    Initialize a distributed process group.

    Args:
        rank (int): A unique process ID.
        world_size (int): Total number of processes in the group.
    """
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12345"

    if platform.system() == "Windows":
        os.environ["USE_LIBUV"] = "0"
        backend = "gloo"
    else:
        backend = "nccl"

    init_process_group(backend=backend, rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


class ToyDataset(Dataset):
    def __init__(self, features: torch.Tensor, labels: torch.Tensor):
        self.features = features
        self.labels = labels

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.features[index], self.labels[index]

    def __len__(self) -> int:
        return self.labels.shape[0]


class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs: int, num_outputs: int):
        super().__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(num_inputs, 30),
            torch.nn.ReLU(),
            torch.nn.Linear(30, 20),
            torch.nn.ReLU(),
            torch.nn.Linear(20, num_outputs),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)


def prepare_dataset() -> Tuple[DataLoader, DataLoader]:
    X_train = torch.tensor([
        [-1.2, 3.1],
        [-0.9, 2.9],
        [-0.5, 2.6],
        [2.3, -1.1],
        [2.7, -1.5]
    ])
    y_train = torch.tensor([0, 0, 0, 1, 1])

    X_test = torch.tensor([
        [-0.8, 2.8],
        [2.6, -1.6],
    ])
    y_test = torch.tensor([0, 1])

    train_ds = ToyDataset(X_train, y_train)
    test_ds = ToyDataset(X_test, y_test)

    train_loader = DataLoader(
        dataset=train_ds,
        batch_size=2,
        shuffle=False,
        pin_memory=True,
        drop_last=True,
        sampler=DistributedSampler(train_ds)
    )
    test_loader = DataLoader(
        dataset=test_ds,
        batch_size=2,
        shuffle=False,
    )
    return train_loader, test_loader


def main(rank: int, world_size: int, num_epochs: int) -> None:
    ddp_setup(rank, world_size)

    train_loader, test_loader = prepare_dataset()
    model = NeuralNetwork(num_inputs=2, num_outputs=2)
    model.to(rank)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)

    model = DDP(model, device_ids=[rank])

    for epoch in range(num_epochs):
        train_loader.sampler.set_epoch(epoch)

        model.train()
        for features, labels in train_loader:
            features, labels = features.to(rank), labels.to(rank)
            logits = model(features)
            loss = F.cross_entropy(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print(f"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}"
                  f" | Batchsize {labels.shape[0]:03d}"
                  f" | Train/Val Loss: {loss:.2f}")

    model.eval()
    train_acc = compute_accuracy(model, train_loader, device=rank)
    print(f"[GPU{rank}] Training accuracy", train_acc)
    test_acc = compute_accuracy(model, test_loader, device=rank)
    print(f"[GPU{rank}] Test accuracy", test_acc)

    destroy_process_group()


def compute_accuracy(model: torch.nn.Module, dataloader: DataLoader, device: int) -> float:
    model.eval()
    correct = 0.0
    total_examples = 0

    with torch.no_grad():
        for features, labels in dataloader:
            features, labels = features.to(device), labels.to(device)
            logits = model(features)
            predictions = torch.argmax(logits, dim=1)
            correct += torch.sum(labels == predictions)
            total_examples += len(labels)

    return (correct / total_examples).item()


if __name__ == "__main__":
    print("PyTorch version:", torch.__version__)
    print("CUDA available:", torch.cuda.is_available())
    print("Number of GPUs available:", torch.cuda.device_count())

    torch.manual_seed(42)

    num_epochs = 3
    world_size = torch.cuda.device_count()
    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size)

```