{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7023224c42b0>>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\"I've been waiting for long time\", \"I hate this so much!\"]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì‚´í´ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í—¤ë“œëŠ” hidden statesì˜ ê³ ì°¨ì› ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹¤ë¥¸ ì°¨ì›ì— íˆ¬ì˜í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í—¤ë“œëŠ” í•˜ë‚˜ ë˜ëŠ” ëª‡ ê°œì˜ ì„ í˜• ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. \n",
    "\n",
    "## 2ì¥ ìš”ì•½\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ì˜ ì‘ë™ ë°©ì‹ê³¼ í† í°í™”, ì…ë ¥ ì‹ë³„ìë¡œì˜ ë³€í™˜, íŒ¨ë”©, ì ˆë‹¨ ë° ì–´í…ì…˜ ë§ˆìŠ¤í¬ë“±ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "\n",
    "# í•´ë‹¹ ì‹œí€€ìŠ¤ë¥¼ ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©(padding) í•©ë‹ˆë‹¤.\n",
    "# model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "# ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ ìµœëŒ€ ê¸¸ì´(model max length)ê¹Œì§€ íŒ¨ë”©(padding) í•©ë‹ˆë‹¤.\n",
    "# (512 for BERT or DistilBERT)\n",
    "# model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ì¥ íŒŒì¸ íŠœë‹\n",
    "\n",
    "ê¸°ì¡´ì— ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì•˜ê³ , ì—¬ëŸ¬ë¶„ë“¤ì´ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  íŒŒì¸íŠœë‹ì„ í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” William B. Dolanê³¼ Chris Brockettì˜ ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ MRPC(Microsoft Research Paraphrase Corpus) ë°ì´í„°ì…‹ì„ ì˜ˆì œë¡œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 5,801ê±´ì˜ ë¬¸ì¥ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê° ë¬¸ì¥ ìŒì˜ ê´€ê³„ê°€ ì˜ì—­(paraphrasing) ê´€ê³„ì¸ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë ˆì´ë¸”ì´ ì¡´ì¬í•©ë‹ˆë‹¤(ì¦‰, ë‘ ë¬¸ì¥ì´ ë™ì¼í•œ ì˜ë¯¸ì¸ì§€ ì—¬ë¶€). ë°ì´í„°ì…‹ì˜ ê·œëª¨ê°€ ê·¸ë¦¬ í¬ì§€ ì•Šê¸° ë•Œë¬¸ì— í•™ìŠµ ê³¼ì •ì„ ì‰½ê²Œ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í—ˆë¸Œ(hub)ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œ(cache) ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì‰¬ìš´ ëª…ë ¹ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ MRPC ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"glue\", \"mrpc\"\n",
    ")  #  MRPC ë°ì´í„°ì…‹ì€ 10ê°œì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±ëœ GLUE ë²¤ì¹˜ë§ˆí¬ ì¤‘ í•˜ë‚˜.\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë ˆì´ë¸”(label)ì€ ClassLabel íƒ€ì…ì´ê³  ë ˆì´ë¸” ì´ë¦„ì— ëŒ€í•œ ì •ìˆ˜ ë§¤í•‘ì€ names í´ë”ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 0ì€ not_equivalentë¥¼ ì˜ë¯¸í•˜ê³ , 1ì€ equivalentë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "## ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "\n",
    "ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” ìš°ì„ ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ì¥ì—ì„œ ë³´ì•˜ë“¯ì´ ì´ëŠ” í† í¬ë‚˜ì´ì €ê°€ ë‹´ë‹¹í•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì— ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë‹¤ì¤‘ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ê° ìŒì˜ ëª¨ë“  ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ ê°ê° ì§ì ‘ í† í°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:00<00:00, 11558.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ í•´ì•¼ í•  ì¼ì€ ì „ì²´ ìš”ì†Œë“¤ì„ ë°°ì¹˜(batch)ë¡œ ë¶„ë¦¬í•  ë•Œ ê°€ì¥ ê¸´ ìš”ì†Œì˜ ê¸¸ì´ë¡œ ëª¨ë“  ì˜ˆì œë¥¼ ì±„ìš°ëŠ”(padding) ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ë™ì  íŒ¨ë”©(dynamic padding)ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ë™ì  íŒ¨ë”©(Dynamic padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ (Training)\n",
    "\n",
    "Trainerë¥¼ ì •ì˜í•˜ê¸° ì „ì— ë¨¼ì € ìˆ˜í–‰í•  ë‹¨ê³„ëŠ” Trainerê°€ í•™ìŠµ ë° í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameters)ë¥¼ í¬í•¨í•˜ëŠ” TrainingArguments í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.30944098124230524, metrics={'train_runtime': 38.2818, 'train_samples_per_second': 287.447, 'train_steps_per_second': 35.97, 'total_flos': 405114969714960.0, 'train_loss': 0.30944098124230524, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "# í´ë˜ìŠ¤ ì •ì˜\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# trainer ì •ì˜\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í‰ê°€ (Evaluation)\n",
    "ìœ ìš©í•œ compute_metrics() í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê³  ì´ë¥¼ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460902</td>\n",
       "      <td>0.732843</td>\n",
       "      <td>0.835596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.451315</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.913495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368500</td>\n",
       "      <td>0.563403</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.905594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.40370349426934404, metrics={'train_runtime': 42.747, 'train_samples_per_second': 257.421, 'train_steps_per_second': 32.213, 'total_flos': 405114969714960.0, 'train_loss': 0.40370349426934404, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer classì•ˆì“°ê³  ê°™ì€ ê²°ê³¼ë¥¼ ì–»ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/1377 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8382352941176471, 'f1': 0.89}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4ì¥. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê³µìœ \n",
    "\n",
    "# 5ì¥. Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "## ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë”©í•˜ê¸°\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ì˜ˆì œ ë°ì´í„°ë¡œ ì´íƒˆë¦¬ì•„ì–´ ì§ˆì˜ì‘ë‹µ(question answering)ì„ ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì¸ SQuAD-it datasetì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 161297 examples [00:00, 373782.24 examples/s]\n",
      "Generating test split: 53766 examples [00:00, 361096.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"../input/drugsComTrain_raw.tsv\", \"test\": \"../input/drugsComTest_raw.tsv\"}\n",
    "# \\t ëŠ” Pythonì—ì„œ íƒ­ ë¬¸ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ìŠ¬ë¼ì´ì‹±(slicing)ê³¼ ë‹¤ì´ì‹±(dicing)\n",
    "\n",
    "Pandasì™€ ìœ ì‚¬í•˜ê²Œ ğŸ¤—DatasetsëŠ” Dataset ë° DatasetDict ê°ì²´ì˜ ë‚´ìš©ì„ ì¡°ì‘í•˜ëŠ” ì—¬ëŸ¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# ì•ìª½ì˜ ìƒ˜í”Œ ëª‡ê°œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¬í˜„ì„±(reproducibility)ì„ ìœ„í•´ Dataset.shuffle()ì˜ seedë¥¼ ê³ ì •í–ˆìŠµë‹ˆë‹¤. Dataset.select()ëŠ” ë°˜ë³µ ê°€ëŠ¥ ì¸ë±ìŠ¤(iterable indices)ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì…ë ¥í•´ì•¼ í•˜ë¯€ë¡œ ë°ì´í„°ì…‹ì—ì„œ ì²˜ìŒ 1,000ê°œì˜ ì˜ˆì œë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ range(1000)ì„ ì „ë‹¬í–ˆìŠµë‹ˆë‹¤. ì´ ìƒ˜í”Œë“¤ì—ì„œ ìš°ë¦¬ëŠ” í•´ë‹¹ ë°ì´í„°ì…‹ì˜ ëª‡ ê°€ì§€ ë‹¨ì (quirks)ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- \"Unnamed: 0\" ì»¬ëŸ¼(column)ì€ í™•ì‹¤í•˜ì§€ëŠ” ì•Šì§€ë§Œ ê° í™˜ìì˜ ìµëª… ID(anonymized ID)ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.\n",
    "- \"condition\" ì»¬ëŸ¼(column)ì—ëŠ” ëŒ€ë¬¸ìì™€ ì†Œë¬¸ì ë ˆì´ë¸”ì´ í˜¼í•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "- ë¦¬ë·°(review)ì˜ ê¸¸ì´ëŠ” ë‹¤ì–‘í•˜ë©° Python ì¤„ êµ¬ë¶„ ê¸°í˜¸(\\r\\n)ì™€ 'ì™€ ê°™ì€ HTML ë¬¸ì ì½”ë“œê°€ í˜¼í•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ğŸ¤—Datasetsë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 161297/161297 [00:00<00:00, 328154.86 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53766/53766 [00:00<00:00, 331288.33 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160398/160398 [00:06<00:00, 26529.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53471/53471 [00:02<00:00, 26483.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "# ê²°ì¸¡ì¹˜ ì œê±°\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# ì†Œë¬¸ìí™”ê°€ ì œëŒ€ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸.\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìƒˆë¡œìš´ ì»¬ëŸ¼(column) ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160398/160398 [00:04<00:00, 33152.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53471/53471 [00:01<00:00, 32796.55 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# ì²« í•™ìŠµ ì˜ˆì œë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160398/160398 [00:00<00:00, 296854.58 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53471/53471 [00:00<00:00, 303639.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset.filter() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 30ë‹¨ì–´ ë¯¸ë§Œìœ¼ë¡œ í‘œí˜„ëœ ë¦¬ë·°ë¥¼ ì œê±°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138514/138514 [00:02<00:00, 47611.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46108/46108 [00:00<00:00, 47460.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ë¦¬ë·°ì— HTML ë¬¸ì ì½”ë“œê°€ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. Pythonì˜ html ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì´ëŸ¬í•œ ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ í•´ì œ(unescape)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])},\n",
    "                                batched=True # ì†ë„ë¥¼ ìœ„í•´\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²€ì¦ ì§‘í•©(validation set) ìƒì„±\n",
    "ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í‰ê°€ ì§‘í•©(test set)ì´ ìˆì§€ë§Œ ê°œë°œ ì¤‘ì— ì´ í‰ê°€ ì§‘í•©ì„ ê·¸ëŒ€ë¡œ ë‘ê³  ë³„ë„ì˜ ê²€ì¦ ì§‘í•©(validation set)ì„ ë§Œë“œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# ê¸°ë³¸ \"test\" ë¶„í• ì„ \"validation\"ìœ¼ë¡œ ë³€ê²½í•¨.\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# 'DatasetDict'ì— \"test\" ì§‘í•©ì„ ì¶”ê°€.\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetsìœ¼ë¡œ ë¹…ë°ì´í„° ë¬¸ì œë¥¼ í•´ê²°\n",
    "\n",
    "### pileì´ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "Pileì€Â [EleutherAI](https://www.eleuther.ai/)ê°€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë§Œë“  ì˜ì–´ í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ì…ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” í•™ìˆ  ë…¼ë¬¸(scientific articles), GitHub ì½”ë“œ ë¦¬í¬ì§€í† ë¦¬ ë° í•„í„°ë§ëœ ì›¹ í…ìŠ¤íŠ¸ì— ì´ë¥´ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ëŠ”Â [14GB ì²­í¬](https://the-eye.eu/public/AI/pile/)ë¡œ ì œê³µë˜ë©°Â [ê°œë³„ì ì¸ êµ¬ì„± ìš”ì†Œ](https://the-eye.eu/public/AI/pile_preliminary_components/)ë¥¼ ê°ê° ë‹¤ìš´ë¡œë“œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pubmed_dataset = load_dataset(\"casinca/PUBMED_title_abstracts_2019_baseline\", split=\"train\", streaming=True)\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age. Systematic review of the published literature. Out-patient clinics, emergency departments and hospitalisation wards in 23 health centres from 10 countries. Cohort studies reporting the frequency of hypoxaemia in children under 5 years of age with ALRI, and the association between hypoxaemia and the risk of dying. Prevalence of hypoxaemia measured in children with ARI and relative risks for the association between the severity of illness and the frequency of hypoxaemia, and between hypoxaemia and the risk of dying. Seventeen published studies were found that included 4,021 children under 5 with acute respiratory infections (ARI) and reported the prevalence of hypoxaemia. Out-patient children and those with a clinical diagnosis of upper ARI had a low risk of hypoxaemia (pooled estimate of 6% to 9%). The prevalence increased to 31% and to 43% in patients in emergency departments and in cases with clinical pneumonia, respectively, and it was even higher among hospitalised children (47%) and in those with radiographically confirmed pneumonia (72%). The cumulated data also suggest that hypoxaemia is more frequent in children living at high altitude. Three papers reported an association between hypoxaemia and death, with relative risks varying between 1.4 and 4.6. Papers describing predictors of hypoxaemia have focused on clinical signs for detecting hypoxaemia rather than on identifying risk factors for developing this complication. Hypoxaemia is a common and potentially lethal complication of ALRI in children under 5, particularly among those with severe disease and those living at high altitude. Given the observed high prevalence of hypoxaemia and its likely association with increased mortality, efforts should be made to improve the detection of hypoxaemia and to provide oxygen earlier to more children with severe ALRI.'},\n",
       " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
       "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy.\\nHypoxaemia is a common complication of acute lower respiratory tract infections in children. In most developing countries, where the majority of deaths from pneumonia occur, facilities for early detection of hypoxaemia are lacking and oxygen is in short supply. This review examines the usefulness of different clinical signs and symptoms in the prediction of hypoxaemia associated with acute respiratory infections in children. Several respiratory signs were found to be associated with hypoxaemia. These include very fast breathing (with a respiratory rate of more than 60 or 70 breaths per minute), cyanosis, grunting, nasal flaring, chest retractions, head nodding and auscultatory signs, as well as signs of general depression of the child, such as inability to feed or lethargy. The sensitivity and specificity of these signs, as described in the reviewed studies, is presented, and combination rules are discussed. Through appropriate combination of several physical signs, which can be used by peripheral health workers and be taught to mothers, it is possible to predict hypoxaemia in children with acute respiratory tract infections with reasonable accuracy.'},\n",
       " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
       "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea.\\nTo investigate the severity and duration of hypoxaemia in 703 children with severe or very severe pneumonia presenting to Goroka Hospital in the Papua New Guinea highlands; to study the predictive value of clinical signs for the severity of hypoxaemia, the predictive value of transcutaneous oxygen saturation (SpO2) and other variables for mortality. Prospective evaluation of children with severe or very severe pneumonia. SpO2 was measured at the time of presentation and every day until hypoxaemia resolved. Children with a SpO2 less than 85% received supplemental oxygen. By comparing with a retrospective control group for whom oxygen administration was guided by clinical signs, we evaluated whether there was a survival advantage from using a protocol for the administration of oxygen based on pulse oximetry. We determined normal values for oxygen saturation in children living in the highlands. In 151 well, normal highland children, the mean SpO2 was 95.7% (SD 2.7%). The median SpO2 among children with severe or very severe pneumonia was 70% (56-77); 376 (53.5%) had moderate hypoxaemia (SpO2 70-84%); 202 (28.7%) had severe hypoxaemia (SpO2 50-69%); and 125 (17.8%) had very severe hypoxaemia (SpO2 < 50%). Longer duration of cough or the presence of hepatomegaly or cyanosis predicted more severe degrees of hypoxaemia. After 10, 20 and 30 days from the beginning of treatment, respectively 102 (14.5%), 38 (5.4%) and 19 (2.7%) of children had persistent hypoxaemia; 46 children (6.5%) died. Predictors of death were low SpO2 on presentation, severe malnutrition, measles and history of cough for more than 7 days. The mortality risk ratio between the 703 children managed whose oxygen administration was guided by the use of pulse oximetry and the retrospective control group who received supplemental oxygen based on clinical signs was 0.65 (95%CI 0.41-1.02, two-sided Fisher's exact test, P = 0.07). There is a need to increase the availability of supplemental oxygen in smaller health facilities in developing countries, and to train health workers to recognise the clinical signs and risk factors for hypoxaemia. In moderate sized hospitals a protocol for the administration of oxygen based on pulse oximetry may improve survival.\"},\n",
       " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
       "  'text': 'Oxygen concentrators and cylinders.\\nA comparison is made between oxygen cylinders and oxygen concentrators as sources for clinical use. Oxygen cylinders are in widespread use, but costs and logistic factors favour the use of concentrators in many developing country situations, especially where cylinder supplies fail to penetrate.'},\n",
       " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
       "  'text': 'Oxygen supply in rural africa: a personal experience.\\nOxygen is one of the essential medical supplies in any hospital setting. However, in some rural African settings without regular electricity and with logistical problems in the transport of oxygen cylinders, the delivery of oxygen can be difficult. This paper compares the costs incurred using oxygen cylinders with a solar operated oxygen concentrator in a rural Gambian setting. The paper shows that a solar operated system has a high capital investment initially, but that running costs and maintenance are minimal. The system becomes cost effective if a rural hospital needs more than 6 treatment days (1 l/min) of oxygen per month and can be considered in a setting where 6 hours of sunlight per day can be guaranteed.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = pubmed_dataset.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìì‹ ë§Œì˜ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°\n",
    "\n",
    "ì¼ë‹¨ì€ ìŠ¤í‚µí•œë‹¤.\n",
    "\n",
    "## FAISSë¥¼ ì´ìš©í•œ ì‹œë§¨í‹± ê²€ìƒ‰\n",
    "\n",
    "### ì‹œë§¨í‹± ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ì‚¬ìš©í•˜ê¸°\n",
    "1ì¥ì—ì„œ ë³´ì•˜ë“¯ì´ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ë‚´ì˜ ê° í† í°ì„ ì„ë² ë”© ë²¡í„°(embedding vector) ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°œë³„ ì„ë² ë”©ì„ \"í’€ë§(pooling)\"í•˜ì—¬ ì „ì²´ ë¬¸ì¥, ë‹¨ë½ ë˜ëŠ” (ê²½ìš°ì— ë”°ë¼) ë¬¸ì„œì— ëŒ€í•œ ë²¡í„° í‘œí˜„ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê° ì„ë² ë”© ì‚¬ì´ì˜ ë‚´ì  ìœ ì‚¬ë„(dot-product similarity), ë˜ëŠ” ë‹¤ë¥¸ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­(similarity metric)ì„ ê³„ì‚°í•˜ê³  ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ì—¬ ì½”í¼ìŠ¤ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ì„ ê°œë°œí•  ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²€ìƒ‰ ì—”ì§„ì€ ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì— ë¹„í•´ ëª‡ ê°€ì§€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"../input/drugsComTrain_raw.tsv\", \"test\": \"../input/drugsComTest_raw.tsv\"}\n",
    "# \\t ëŠ” Pythonì—ì„œ íƒ­ ë¬¸ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì œê±°\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['drugName', 'condition'],\n",
       "    num_rows: 160398\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset.remove_columns() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ì™¸ì˜ ë‚˜ë¨¸ì§€ ì—´ì„ ì‚­ì œí•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "dataset = drug_dataset[\"train\"]\n",
    "columns = dataset.column_names\n",
    "columns_to_keep = [\"drugName\", \"condition\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"drugName\"] \n",
    "        + \" \\n \" \n",
    "        + examples[\"condition\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(concatenate_text)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embedding = get_embeddings(dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160398/160398 [07:32<00:00, 354.41 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 161/161 [00:00<00:00, 408.53it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'comments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16936/2907470400.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m embeddings_dataset = dataset.map(\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0membeddings_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the best drug in each condition?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'comments'"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "question = \"What is the best drug in each condition?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "# question_embedding.shape\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "- ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì˜ í† í¬ë‚˜ì´ì €ì™€ ìœ ì‚¬í•œ ìƒˆë¡œìš´ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•\n",
    "- ì˜¤ëŠ˜ë‚  NLPì—ì„œ ì‚¬ìš©ë˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ì–´ í† í°í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì°¨ì´ì \n",
    "- ğŸ¤—Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ìŒë¶€í„° í† í¬ë‚˜ì´ì €ë¥¼ êµ¬ì¶•í•˜ê³  íŠ¹ì • ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì—ì„œ ìƒˆë¡œìš´ í† í¬ë‚˜ì´ì € í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ë¡œë“œí•˜ëŠ”ë° ëª‡ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì»¤í”¼ë‚˜ ì°¨ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# ì´ì œ í…ìŠ¤íŠ¸ ë°°ì¹˜(batch)ì˜ ì´í„°ë ˆì´í„°(iterator) í˜•íƒœë¡œ ë§ë­‰ì¹˜ë¥¼ êµ¬ì„±\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "\n",
    "# ëª¨ë¸ê³¼ ì¼ì¹˜ì‹œí‚¤ë ¤ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•´ì•¼\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# train_new_from_iterator() ì‚¬ìš© ì¤‘ì¸ í† í¬ë‚˜ì´ì €ê°€ \"ë¹ ë¥¸(fast)\" í† í¬ë‚˜ì´ì €ì¸ ê²½ìš°ì—ë§Œ ì‘ë™í•©\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "# tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old tokenizer result:\n",
      "['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n",
      "\n",
      "New tokenizer result:\n",
      "['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']\n"
     ]
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "old_tokens = old_tokenizer.tokenize(example)\n",
    "new_tokens = tokenizer.tokenize(example)\n",
    "\n",
    "print(\"Old tokenizer result:\")\n",
    "print(old_tokens)\n",
    "print(\"\\nNew tokenizer result:\")\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"ë¹ ë¥¸(fast)\" í† í¬ë‚˜ì´ì €ì˜ íŠ¹ë³„í•œ ëŠ¥ë ¥\n",
    "\n",
    "### ë°°ì¹˜ ì¸ì½”ë”© (Batch encoding)\n",
    "\n",
    "### ì…ë ¥(inputs)ì—ì„œ ì˜ˆì¸¡(predictions)ê¹Œì§€\n",
    "\n",
    "### ì—”í„°í‹° ê·¸ë£¹í™”\n",
    "\n",
    "## QA íŒŒì´í”„ë¼ì¸ì—ì„œì˜ \"ë¹ ë¥¸(fast)\" í† í¬ë‚˜ì´ì €\n",
    "\n",
    "ì§ˆì˜ ì‘ë‹µ(question answering) ì‘ì—…ì— ê´€ì‹¬ì´ ì—†ë‹¤ë©´ ì´ ì„¹ì…˜ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ì •ê·œí™”(Normalization) ë° ì‚¬ì „ í† í°í™”(Pre-tokenization)\n",
    "\n",
    "í…ìŠ¤íŠ¸ë¥¼ í•˜ìœ„ í† í°(subtokens)ìœ¼ë¡œ ë¶„í• í•˜ê¸° ì „ì—(ëª¨ë¸ì— ë”°ë¼), í† í¬ë‚˜ì´ì €ëŠ” ì •ê·œí™”(normalization) ë° ì‚¬ì „ í† í°í™”(pre-tokenization) ë‘ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì •ê·œí™”(Normalization)\n",
    "ì •ê·œí™” ë‹¨ê³„ì—ëŠ” ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜(lowercasing) ë° ì•…ì„¼íŠ¸ ì œê±° ë“±ê³¼ ê°™ì€ ëª‡ê°€ì§€ ì¼ë°˜ì ì¸ ì •ì œ ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "### ì‚¬ì „í† í°í™”(Pre-tokenization)\n",
    "\n",
    "í† í¬ë‚˜ì´ì €ëŠ” ì›ì‹œ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œëŠ” í•™ìŠµë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹ ì— ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ì™€ ê°™ì€ ì‘ì€ ê°œì²´ë“¤ë¡œ ë¶„í• í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì‚¬ì „ í† í°í™”(pre-tokenization) ë‹¨ê³„ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤. \n",
    "\n",
    "### SentencePiece\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í† í°í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì¼ë ¨ì˜ ìœ ë‹ˆì½”ë“œ ë¬¸ìë“¤ë¡œ ê°„ì£¼í•˜ê³  ê³µë°±ì„ íŠ¹ìˆ˜ ë¬¸ìì¸ _ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤. Unigram ì•Œê³ ë¦¬ì¦˜(ì„¹ì…˜ 7 ì°¸ì¡°)ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì‚¬ì „ í† í°í™”(pre-tokenization) ë‹¨ê³„ê°€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê³µë°± ë¬¸ìê°€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì–¸ì–´(ì˜ˆ: ì¤‘êµ­ì–´ ë˜ëŠ” ì¼ë³¸ì–´)ì— ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "## Byte-Pair Encoding (BPE) í† í°í™”\n",
    "\n",
    " í† í°í™” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ê°œìš”ë§Œì„ ì›í•˜ëŠ” ê²½ìš° ì´ ì¥ì„ ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤.\n",
    "\n",
    "## WordPiece í† í°í™”\n",
    "\n",
    " WordPieceë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë‹¤ë£¨ë©° ì „ì²´ êµ¬í˜„ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í† í°í™” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ê°œìš”ë¥¼ ì›í•˜ëŠ” ê²½ìš° ìƒëµí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    " ## Unigram í† í°í™”\n",
    "\n",
    "  í† í°í™” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ê°œìš”ë¥¼ ì›í•˜ëŠ” ê²½ìš° ìƒëµí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ë¸”ë¡ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì € ë¹Œë”©í•˜ê¸°\n",
    "\n",
    "\n",
    " í† í°í™”ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤:\n",
    "\n",
    "- ì •ê·œí™” (ê³µë°±ì´ë‚˜ ì•…ì„¼íŠ¸ ì œê±°, ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ë“±ê³¼ ê°™ì´ í•„ìš”í•˜ë‹¤ê³  ì—¬ê²¨ì§€ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ ì •ì œ ì‘ì—…)\n",
    "- ì‚¬ì „ í† í°í™” (ì…ë ¥ì„ ë‹¨ì–´ë“¤ë¡œ ë¶„ë¦¬)\n",
    "- ëª¨ë¸ì„ í†µí•œ ì…ë ¥ ì‹¤í–‰ (ì‚¬ì „ í† í°í™”ëœ ë‹¨ì–´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í† í° ì‹œí€€ìŠ¤ ìƒì„±)\n",
    "- í›„ì²˜ë¦¬ (í† í°ë‚˜ì´ì €ì˜ íŠ¹ìˆ˜ í† í° ì¶”ê°€, attention mask ë° í† í° ìœ í˜• ID ìƒì„±)\n",
    "\n",
    "\n",
    "### ë§ë­‰ì¹˜ í™•ë³´\n",
    "\n",
    "ìƒˆë¡œìš´ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‘ì€ í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4358/4358 [00:00<00:00, 337165.93 examples/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:00<00:00, 530895.64 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3760/3760 [00:00<00:00, 1183711.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "# WordPiece í† í¬ë‚˜ì´ì €ë¥¼ ì²˜ìŒë¶€í„° ë¹Œë”©í•˜ê¸°\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "# í† í°í™”ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì •ê·œí™”(normalization)ì…ë‹ˆë‹¤.\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "# ë‹¤ìŒì€ ì‚¬ì „ í† í°í™” ë‹¨ê³„ì…ë‹ˆë‹¤\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ ì „ë‹¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "# ì•ì—ì„œ ì •ì˜í•œ ë°˜ë³µì(iterator)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer\")\n",
    "print(encoding.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì£¼ìš” NLP íƒœìŠ¤í¬ ì‹¤ì œ êµ¬í˜„ë°©ë²•\n",
    "\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” NLP íƒœìŠ¤í¬ë“¤ì„ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.\n",
    "- í† í° ë¶„ë¥˜ (Token Classification)\n",
    "- ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ (Masked Language Modeling)\n",
    "- ìš”ì•½ (Summarization)\n",
    "- ë²ˆì—­ (Translation)\n",
    "- ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ ì‚¬ì „í•™ìŠµ (Causal Language Modeling Pretraining like GPT-2)\n",
    "- ì§ˆì˜ì‘ë‹µ (Question Answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í† í° ë¶„ë¥˜ (Token Classification)\n",
    "\n",
    "ì´ í¬ê´„ì ì¸ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ì´ \"ë¬¸ì¥ì˜ ê° í† í°ì— ë ˆì´ë¸”ì„ ì§€ì •\"í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •í˜•í™”ë  ìˆ˜ ìˆëŠ” ëª¨ë“  ë¬¸ì œë¥¼ í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 36183.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 41512.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 26093.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# ë°ì´í„° ì²˜ë¦¬\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # ìƒˆë¡œìš´ ë‹¨ì–´ì˜ ì‹œì‘ í† í°.\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # íŠ¹ìˆ˜ í† í°.\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # ì´ì „ í† í°ê³¼ ë™ì¼í•œ ë‹¨ì–´ì— ì†Œì†ëœ í† í°.\n",
    "            label = labels[word_id]\n",
    "            # ë§Œì•½ ë ˆì´ë¸”ì´ B-XXXì´ë©´ ì´ë¥¼ I-XXXë¡œ ë³€ê²½.\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# examplesëŠ” ë‹¨ì¼ í…ìŠ¤íŠ¸(ë¬¸ì¥)ê°€ ì•„ë‹ˆë¼ ë‹¤ì¤‘ í…ìŠ¤íŠ¸ì„.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # ë°°ì¹˜(batch) ì¸ë±ìŠ¤ ì§€ì •\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Trainer APIë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ ë¯¸ì„¸ì¡°ì •\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í‰ê°€ ê¸°ì¤€ (Metrics) \n",
    "\n",
    "ì´í›„ ìƒëµ\n",
    "\n",
    "## ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸(Masked Language Model) ë¯¸ì„¸ì¡°ì •\n",
    "\n",
    "ë¶„ì•¼ íŠ¹í™” ë°ì´í„°ì— ëŒ€í•´ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ë„ë©”ì¸ ì–´ëí…Œì´ì…˜(domain adaptation)ì´ë¼ê³  í•©ë‹ˆë‹¤. \n",
    "\n",
    "### ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§(MLM)ì„ ìœ„í•´ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸(pretrained model) ì„ íƒí•˜ê¸°\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 200707.83 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 442808.94 examples/s]\n",
      "Generating unsupervised split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00<00:00, 515089.78 examples/s]\n",
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:02<00:00, 11886.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:01<00:00, 12986.14 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:04<00:00, 12211.42 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:19<00:00, 1264.06 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:18<00:00, 1327.32 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:40<00:00, 1249.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í† í°ì„ ì˜ˆì¸¡í•˜ë ¤ë©´ ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ì„ ìƒì„±í•˜ê¸° ìœ„í•´ DistilBERTì˜ í† í¬ë‚˜ì´ì €ê°€ í•„ìš”\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "#ë°ì´í„°ì…‹\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# ë¹ ë¥¸ ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‘ë™ì‹œí‚¤ê¸° ìœ„í•´ì„œ, batched=Trueë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì•½ê°„ ì‘ì€ ìˆ˜ì¹˜ë¥¼ ì„ íƒ\n",
    "chunk_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # ëª¨ë“  í…ìŠ¤íŠ¸ë“¤ì„ ê²°í•©í•œë‹¤.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # ê²°í•©ëœ í…ìŠ¤íŠ¸ë“¤ì— ëŒ€í•œ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # `chunk_size`ë³´ë‹¤ ì‘ì€ ê²½ìš° ë§ˆì§€ë§‰ ì²­í¬ë¥¼ ì‚­ì œ\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # max_len ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” chunk ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ìŠ¤\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # ìƒˆë¡œìš´ ë ˆì´ë¸” ì»¬ëŸ¼ì„ ìƒì„±\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result                            \n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "# Trainer APIë¥¼ ì´ìš©í•˜ì—¬ DistilBERT ë¯¸ì„¸ì¡°ì •\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 50_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 11.06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2346' max='2346' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2346/2346 02:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.496500</td>\n",
       "      <td>2.333867</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.424800</td>\n",
       "      <td>2.296707</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.391800</td>\n",
       "      <td>2.276380</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 9.97\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    # overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.train()\n",
    "\n",
    "eval_results  = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë²ˆì—­\n",
    "\n",
    "ë²ˆì—­(translation)ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤. ì´ê²ƒì€ ë˜ ë‹¤ë¥¸ í˜•íƒœì˜ sequence-to-sequence íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ì¦‰, í•œ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë¡œ ì´ë™í•˜ëŠ”(ë³€í˜•í•˜ëŠ”) ê²ƒì´ì£ . ê·¸ëŸ° ì˜ë¯¸ì—ì„œ ì´ ë¬¸ì œëŠ” ìš”ì•½(summarization)ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ê³  ì—¬ê¸°ì—ì„œ ë³´ê²Œ ë  ë‚´ìš©ì„ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë¥¸ sequence-to-sequence ë¬¸ì œì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- ìŠ¤íƒ€ì¼ íŠ¸ëœìŠ¤í¼(style transfer): íŠ¹ì • ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ë²ˆì—­í•˜ëŠ” ëª¨ë¸ ìƒì„±(ì˜ˆ: ê²©ì‹ ìŠ¤íƒ€ì¼ì—ì„œ ìºì£¼ì–¼ ìŠ¤íƒ€ì¼ë¡œ ë˜ëŠ” ì…°ìµìŠ¤í”¼ì–´ ì˜ì–´ì—ì„œ í˜„ëŒ€ ì˜ì–´ë¡œ)\n",
    "- ìƒì„± ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ(generative question answering): ì£¼ì–´ì§„ ë§¥ë½(context)ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°\n",
    "\n",
    "ë‘ ê°œ(ë˜ëŠ” ê·¸ ì´ìƒ) ì–¸ì–´ë¡œ ëœ ì¶©ë¶„íˆ í° í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ê°€ ìˆëŠ” ê²½ìš° ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§(causal language modeling) ì„¹ì…˜ì—ì„œ í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì²˜ìŒë¶€í„° ìƒˆë¡œìš´ ë²ˆì—­ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ì • ê·œëª¨ì˜ íŠ¹ì • ì–¸ì–´ë¡œ í‘œí˜„ëœ ë§ë­‰ì¹˜ë¥¼ ê°€ì§€ê³  mT5ë‚˜ mBART ë“±ê³¼ ê°™ì€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë‹¤êµ­ì–´ ë²ˆì—­ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” í¸ì´ í›¨ì”¬ ë¹ ë¥¼ ê²ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ë²ˆì—­ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê±°ë‚˜ ì²˜ìŒë¶€í„° ì‚¬ì „ í•™ìŠµí•˜ë ¤ë©´ ì‘ì—…ì— ì í•©í•œ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤. KDE4 ë°ì´í„°ì…‹ì„ `load_dataset()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en': 'Publisher', 'fr': 'Ã‰diteur'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# from evaluate import load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì²˜ë¦¬\n",
    "\n",
    "ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í† í° ID ì„¸íŠ¸ë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì„ ìœ„í•´ ì…ë ¥ê³¼ íƒ€ê²Ÿì„ ëª¨ë‘ í† í°í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ì‘ì—…ì€ tokenizer ê°ì²´ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Marian English to French ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì–¸ì–´ ìŒì— ëŒ€í•´ì„œ ì‘ì—…í•˜ë ¤ë©´ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤. Helsinki-NLP ì¡°ì§ì€ ì—¬ëŸ¬ ì–¸ì–´ë¡œ ì²œ ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map:   0%|          | 0/189155 [00:00<?, ? examples/s]/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189155/189155 [00:10<00:00, 18088.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21018/21018 [00:01<00:00, 17995.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "# mBART, mBART-50 ë˜ëŠ” M2M100ê³¼ ê°™ì€ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° tokenizer.src_lang ë° tokenizer.tgt_langì„ ì˜¬ë°”ë¥¸ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í† í¬ë‚˜ì´ì €ì—ì„œ ì…ë ¥ ë° ëŒ€ìƒì˜ ì–¸ì–´ ì½”ë“œë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ê°€ ì „ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer APIë¡œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°\n",
    "\n",
    "ì—¬ê¸°ì—ì„œëŠ” Seq2SeqTrainerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Seq2SeqTrainerëŠ” Trainerì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¡œì„œ ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ generate() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¥¼ ì ì ˆí•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì € ë¯¸ì„¸ ì¡°ì •í•  ì‹¤ì œ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ AutoModel APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì½œë ˆì´ì…˜ (Data Collation)\n",
    "ë™ì  ë°°ì¹˜ ì²˜ë¦¬(Dynamic batching)ë¥¼ ìœ„í•œ íŒ¨ë”©ì„ ì²˜ë¦¬í•˜ë ¤ë©´ ë°ì´í„° ì½œë ˆì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ í‰ê°€ ì§€í‘œ(metric)ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### í‰ê°€ì§€í‘œ (Metrics)\n",
    "\n",
    "Seq2SeqTrainerê°€ ìˆ˜í¼í´ë˜ìŠ¤ Trainerì— ì¶”ê°€í•˜ëŠ” ê¸°ëŠ¥ì€ í‰ê°€(evaluation) ë˜ëŠ” ì˜ˆì¸¡(prediction) ì¤‘ì— generate() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # ëª¨ë¸ì´ ì˜ˆì¸¡ ë¡œì§“(logits)ì™¸ì— ë‹¤ë¥¸ ê²ƒì„ ë¦¬í„´í•˜ëŠ” ê²½ìš°.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # -100ì€ ê±´ë„ˆë›´ë‹¤.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ë‹¨ìˆœ í›„ì²˜ë¦¬\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ trainerë¡œ ì „ë‹¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='658' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/329 21:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.697405457496643,\n",
       " 'eval_model_preparation_time': 0.0014,\n",
       " 'eval_bleu': 39.41460649639242,\n",
       " 'eval_runtime': 340.641,\n",
       " 'eval_samples_per_second': 61.701,\n",
       " 'eval_steps_per_second': 0.966}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17736' max='17736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17736/17736 10:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.835200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.820300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8628926873207092,\n",
       " 'eval_model_preparation_time': 0.0014,\n",
       " 'eval_bleu': 53.21851308777082,\n",
       " 'eval_runtime': 355.5299,\n",
       " 'eval_samples_per_second': 59.117,\n",
       " 'eval_steps_per_second': 0.925,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìš”ì•½ (Summarization)\n",
    "\n",
    "Transformer ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ì„œë¥¼ ê°„ëµí•˜ê²Œ ì••ì¶•í•˜ëŠ” ë°©ë²•, ì¦‰ í…ìŠ¤íŠ¸ ìš”ì•½(text summarization) íƒœìŠ¤í¬ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì€ ê°€ì¥ ì–´ë ¤ìš´ NLP ì‘ì—… ì¤‘ í•˜ë‚˜ë¡œ ì•Œë ¤ì ¸ ìˆëŠ”ë° ê·¸ ì´ìœ ëŠ” ê¸¸ì´ê°€ ê¸´ êµ¬ì ˆì„ ì´í•´í•˜ê³  ì „ì²´ ë¬¸ì„œì˜ í•µì‹¬ ì£¼ì œë¥¼ í¬ê´„í•˜ëŠ” ì¼ê´€ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ëŠ¥ë ¥ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ë‹¤ì¤‘ ì–¸ì–´ ë§ë­‰ì¹˜ ì¤€ë¹„í•˜ê¸°\n",
    "\n",
    "Multilingual Amazon Reviews Corpusë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì¤‘ ì–¸ì–´ ìš”ì•½ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë§ë­‰ì¹˜ëŠ” 6ê°œ ì–¸ì–´ë¡œ ëœ Amazon ì œí’ˆ ë¦¬ë·°ë¡œ êµ¬ì„±ë˜ë©° ì¼ë°˜ì ìœ¼ë¡œ ë‹¤êµ­ì–´ ë¶„ë¥˜ê¸°ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê° ë¦¬ë·°ì—ëŠ” ì§§ì€ ì œëª©ì´ ìˆ˜ë°˜ë˜ë¯€ë¡œ ì´ ì œëª©ì„ ëª¨ë¸ì´ í•™ìŠµí•  ëŒ€ìƒ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefunctDatasetError",
     "evalue": "Dataset 'amazon_reviews_multi' is defunct and no longer accessible due to the decision of data providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefunctDatasetError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m spanish_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamazon_reviews_multi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m english_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_reviews_multi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# english_dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆëŠ” ë¦¬ë·° ì •ë³´ëŠ” review_body ë° review_title ì—´ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 5ì¥ì—ì„œ ë°°ìš´ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì§‘í•©ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ì£ .\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/datasets/load.py:1890\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1888\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1890\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/datasets/builder.py:353\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_exported_dataset_info()\n\u001b[0;32m--> 353\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    354\u001b[0m info\u001b[38;5;241m.\u001b[39mbuilder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    355\u001b[0m info\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/amazon_reviews_multi/30d298e0990e8a7e143dc917665441883d7e0b6a64516ef7eea2e89c1af1755c/amazon_reviews_multi.py:91\u001b[0m, in \u001b[0;36mAmazonReviewsMulti._info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_info\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DefunctDatasetError(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon_reviews_multi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is defunct and no longer accessible due to the decision of data providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mDatasetInfo(\n\u001b[1;32m     95\u001b[0m         description\u001b[38;5;241m=\u001b[39m_DESCRIPTION,\n\u001b[1;32m     96\u001b[0m         features\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mFeatures(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m         citation\u001b[38;5;241m=\u001b[39m_CITATION,\n\u001b[1;32m    112\u001b[0m     )\n",
      "\u001b[0;31mDefunctDatasetError\u001b[0m: Dataset 'amazon_reviews_multi' is defunct and no longer accessible due to the decision of data providers"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n",
    "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
    "# english_dataset\n",
    "# ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆëŠ” ë¦¬ë·° ì •ë³´ëŠ” review_body ë° review_title ì—´ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n",
    "# 5ì¥ì—ì„œ ë°°ìš´ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì§‘í•©ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ê°€ì ¸ì˜¤ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ì£ .\n",
    "\n",
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
    "        print(f\"'>> Review: {example['review_body']}'\")\n",
    "\n",
    "# show_samples(english_dataset)\n",
    "# Amazonì˜ ëŒ€í‘œì ì¸ í…Œë§ˆ ìƒí’ˆì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ ì„œí‰ ìš”ì•½(book review)ì— ì§‘ì¤‘í•©ì‹œë‹¤.\n",
    "def filter_books(example):\n",
    "    return (\n",
    "        example[\"product_category\"] == \"book\"\n",
    "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
    "    )\n",
    "\n",
    "# í•„í„°ë¥¼ ì ìš©í•˜ê¸° ì „ì— english_dataset í˜•ì‹ì„ \"pandas\"ì—ì„œ \"arrow\"ë¡œ ë‹¤ì‹œ ì „í™˜\n",
    "english_dataset.reset_format()\n",
    "\n",
    "spanish_books = spanish_dataset.filter(filter_books)\n",
    "english_books = english_dataset.filter(filter_books)\n",
    "\n",
    "# ë‘ ê°œì˜ Dataset ê°ì²´ë¥¼ í•©ì¹˜ëŠ” concatenate_datasets() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in english_books.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_books[split], spanish_books[split]]\n",
    "    )\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "\n",
    "# ëª‡ ê°œì˜ ìƒ˜í”Œì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "show_samples(books_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ê°€ ë”ì´ìƒ ë‹¤ìš´ë¡œë“œê°€ ì–´ë µê¸° ë•Œë¬¸ì— ì•„ë˜ ë‚´ìš©ì€ ë”ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ.\n",
    "\n",
    "### ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "\n",
    "ë‹¤ìŒ ì‘ì—…ì€ ë¦¬ë·°ì™€ ì œëª©ì„ í† í°í™”í•˜ê³  ì¸ì½”ë”©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í‰ì†Œì²˜ëŸ¼ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ê²°ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ê°€ê¸‰ì  ì§§ì€ ì‹œê°„ ë‚´ì— ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ mt5-smallì„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "# ëª¨ë¸ì— ì§€ë‚˜ì¹˜ê²Œ ê¸´ ì…ë ¥ì„ ì „ë‹¬í•˜ì§€ ì•Šë„ë¡ ë¦¬ë·°ì™€ ì œëª© ëª¨ë‘ì— ì ˆë‹¨ ì‘ì—…(truncation)ì„ ìˆ˜í–‰\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"review_body\"], max_length=max_input_length, truncation=True\n",
    "    )\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì„¤ì •\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ìš”ì•½ì„ ìœ„í•œ í‰ê°€ì§€í‘œ(metrics)\n",
    "\n",
    "ì´ ì½”ìŠ¤ì—ì„œ ë‹¤ë£¬ ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ ì‘ì—…ê³¼ ë¹„êµí•  ë•Œ ìš”ì•½ ë˜ëŠ” ë²ˆì—­ê³¼ ê°™ì€ í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ê°„ë‹¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv add rouge_score\n",
    "import evaluate\n",
    "rouge_score = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸(Causal Language Model)ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ê¸°\n",
    "\n",
    "ì½”ë“œ ìƒì„± ëª¨ë¸ì˜ ì¶•ì†Œ ë²„ì „ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤. Python ì½”ë“œì˜ í•˜ìœ„ ì§‘í•©ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ëŒ€ì‹  í•œ ì¤„ ì™„ì„±(one-line completions)ì— ì¤‘ì ì„ ë‘˜ ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "### ë°ì´í„° ëª¨ìœ¼ê¸°\n",
    "Python ì½”ë“œëŠ” GitHubì™€ ê°™ì€ ì½”ë“œ ë¦¬í¬ì§€í† ë¦¬ì—ì„œ í’ë¶€í•˜ê²Œ ì œê³µë˜ë©°, ëª¨ë“  Python ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìŠ¤í¬ë©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/dataScience/learn_LLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "# from datasets import Dataset\n",
    "\n",
    "# def any_keyword_in_string(string, keywords):\n",
    "#     for keyword in keywords:\n",
    "#         if keyword in string:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def filter_streaming_dataset(dataset, filters):\n",
    "#     filtered_dict = defaultdict(list)\n",
    "#     total = 0\n",
    "#     for sample in tqdm(iter(dataset)):\n",
    "#         total += 1\n",
    "#         if any_keyword_in_string(sample[\"content\"], filters):\n",
    "#             for k, v in sample.items():\n",
    "#                 filtered_dict[k].append(v)\n",
    "#     print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "#     return Dataset.from_dict(filtered_dict)\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ í•„í„°ë§í•˜ëŠ” ë° ì»´í“¨í„°ì™€ ëŒ€ì—­í­ì— ë”°ë¼ 2-3ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë§ì€ ì‹œê°„ì´ ê±¸ë¦¬ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ìˆ˜í–‰í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´ í—ˆë¸Œì—ì„œ í•„í„°ë§ëœ ë°ì´í„°ì…‹ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid.shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„°ì…‹ ì¤€ë¹„í•˜ê¸°\n",
    "ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ë°ì´í„°ë¥¼ í† í°í™”í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:10<00:00, 709.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 809.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1379373\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 12754\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ ëª¨ë¸ì„ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. \n",
    "\n",
    "### ìƒˆë¡œìš´ ëª¨ë¸ì˜ ì´ˆê¸°í™”\n",
    "ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” GPT-2 ëª¨ë¸ì„ ìƒˆë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# í•´ë‹¹ ì„¤ì •ìœ¼ë¡œ ìƒˆ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "# ì‹¤ì œë¡œ ëª¨ë¸ì„ ì§ì ‘ ì´ˆê¸°í™”í•˜ê¸° ë•Œë¬¸ì— from_pretrained() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ë°°ì¹˜ ìƒì„±ì„ ì²˜ë¦¬í•  ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ DataCollatorForLanguageModeling ì½œë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì„ ì‹¤ì œë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëª¨ë“  ê²ƒì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5388' max='5388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5388/5388 48:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.482400</td>\n",
       "      <td>1.622585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5388, training_loss=2.4245188825645534, metrics={'train_runtime': 2924.4282, 'train_samples_per_second': 471.673, 'train_steps_per_second': 1.842, 'total_flos': 9.0101853978624e+16, 'train_loss': 2.4245188825645534, 'epoch': 0.9999536027467174})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    logging_steps=5000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5000,\n",
    "    fp16=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "# 20ì‹œê°„ ê±¸ë¦´ ìˆ˜ ìˆìŒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµëœ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ë´…ì‹œë‹¤! ë¡œê·¸ì—ì„œ ì†ì‹¤ì´ ê¾¸ì¤€íˆ ê°ì†Œí–ˆìŒì„ ì•Œ ìˆ˜ ìˆì§€ë§Œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ íŠ¹ì • í”„ë¡¬í”„íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create scatter plot with x, y\n",
      "plot(x, y, \"o\", label=\"data\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, logging\n",
    "\n",
    "# Set logging to error level to suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")\n",
    "\n",
    "# Set pad_token_id explicitly\n",
    "pipe.model.config.pad_token_id = pipe.model.config.eos_token_id\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "\n",
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = pipe(txt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
